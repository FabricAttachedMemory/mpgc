/*
 *
 *  Multi Process Garbage Collector
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

#include <condition_variable>
#include <unordered_map>

#include "ruts/cuckoo_map.h"
#include "mpgc/gc_handshake.h"
#include "mpgc/gc_thread.h"
#include "mpgc/gc.h"

namespace mpgc {
  namespace gc_handshake {
    extern void do_sweep_signal();
  }

  volatile bool request_gc_termination = false;
  static std::mutex gc_termination_mutex;
  static std::condition_variable gc_terminated;

  //Used by fault-tolerance code to determine which barrier will be the next one.
  static const Barrier_indices next_barrier_index_mapping[Barrier_indices::arraysize] = {Barrier_indices::preSweep,
                                                                                         Barrier_indices::preMarking,
                                                                                         Barrier_indices::marking1,
                                                                                         Barrier_indices::sweep1,
                                                                                         Barrier_indices::sweep2,
                                                                                         Barrier_indices::postSweep,
                                                                                         Barrier_indices::sync};

  /*
   * Structure to be used by the cleanup functions which loop over processes
   * to determine if there is a dead process, and if so, then take appropriate
   * action.
   */
  template <typename T>
  struct barrier_id_dead_processes_t {
    std::deque<T> deque;
    bool found;
    barrier_id_dead_processes_t() : deque(), found(false) {}
  };
  template <typename T> using barrier_id_dead_processes_map = typename std::unordered_map<pcount_t, barrier_id_dead_processes_t<T>>;

  /* 
   * This function is to be called while looping to allocate a block from global allocator.
   * This is needed as otherwise a thread, which has deferred sweep signal, will never allow
   * sweep to progress, and will loop forever in the global allocator.
   */
  void global_allocation_epilogue() {
    gc_handshake::in_memory_thread_struct &thread_struct = *gc_handshake::thread_struct_handles.handle;

    /* We should continue to defer sweep signal until allocation_epilogue() is invoked.
     * This is to avoid any race that may arise otherwise.
     */
    if (thread_struct.sweep_signal_requested) {
      thread_struct.sweep_signal_requested = false;
      gc_handshake::do_sweep_signal();
    }
  }

  /*
   * This function is called before allocation to defer sweep signal.
   */
  void allocation_prologue() {
    //Initialize the GC in the allocation path.
    initialize_thread();

    gc_handshake::in_memory_thread_struct &thread_struct = *gc_handshake::thread_struct_handles.handle;
    thread_struct.sweep_signal_disabled = true;

    if (thread_struct.clear_local_allocator) {
      thread_struct.clear_local_allocator = false;
      thread_struct.local_free_list.clear();
    }
  }

  /*
   * This function is called after the allocation and before construction.
   */
  void allocation_epilogue(void *p, gc_token &tok, std::size_t array_element_count) {
    gc_handshake::in_memory_thread_struct &thread_struct = *gc_handshake::thread_struct_handles.handle;
    assert(thread_struct.status_idx.load().status() != gc_handshake::Signum::sigInit);
    gc_control_block &cb = control_block();

    /* The following is required to avoid a race condition where the construction of gc_array_base
     * gets pre-empted by async_signal after the construction of gc_allocated, but before initializing
     * the array size.
     */
    *(static_cast<std::size_t*>(p) + 1) = array_element_count;

    /* We need the following signal_fence because the array_size *must* be
     * stored before constructing gc_descriptor. Otherwise, if it gets
     * re-ordered, and we get a async signal; the object size calculation
     * will be wrong.
     */
    std::atomic_signal_fence(std::memory_order_release);

    const offset_ptr<const gc_allocated> ptr = new (p) gc_allocated(tok);

    /* We need the following signal_fence because if we read the status
     * before the store above, get a async signal, and then make the above
     * store, then we will neither mark it, nor will be considered in the
     * stack scanning of the async signal handler.
     */
    std::atomic_signal_fence(std::memory_order_release);

    /*
     * Normally we mark end-bitmap first, followed by begin-bitmap. This way
     * if the process crashes before marking both the bits, we would not have
     * marked the object by then, as marked test is done using begin-bitmap.
     *
     * However, for application threads which are allocating black, we should
     * mark begin-bitmap first, as otherwise, the sweep algorithm doesn't work
     * correctly. However, setting only the end-bitmap (and not the begin-bitmap)
     * is tolerable by the sweep algorithm. Also, marked test is never done on
     * these new allocations which are not even being pointed by any field in the
     * heap.
     */
    if (thread_struct.status_idx.load().status() == gc_handshake::Signum::sigAsync) {
      cb.bitmap.mark_begin_first(ptr);
    }
    cb.mem_stats.marked(ptr);

    /* We need the following signal_fence because sweep signal *must* not be
     * enabled (or processed) before marking, if we are in async phase.
     */
    std::atomic_signal_fence(std::memory_order_release);

    //Enable sweep signal, and process if already pending.
    thread_struct.sweep_signal_disabled = false;
    if (thread_struct.sweep_signal_requested) {
      thread_struct.sweep_signal_requested = false;
      gc_handshake::do_sweep_signal();
    }
  }

  /*
   * Function to capture root pointers, both, external_gc_ptrs and persistent roots.
   */
  static void capture_global_roots(Traversal_queue &q) {
    gc_control_block &cb = control_block();
    inbound_pointers::inbound_table::table(true)->for_each_slot([&q, &cb](const offset_ptr<const gc_allocated> p) {
      if (p.is_valid() && !cb.bitmap.is_marked(p)) {
        //q.push_front(p);
          q.push(p);
      }
    });

    cb.persistent_roots
      .enumerate_pointers([&q, &cb](const gc_ptr<const gc_allocated> &r) {
          if (r != nullptr) {
            offset_ptr<const gc_allocated> p = r.as_offset_pointer();
            if (p.is_valid()  && !cb.bitmap.is_marked(p)) {
              //q.push_front(p);
              q.push(p);
            }
          }
        });
  }

  /*
   * The main function that marks black an object. It enumerates all the pointers in the object and
   * then marks it.
   */
  static void mark_black(const offset_ptr<const gc_allocated> &p, gc_control_block &cb, Traversal_queue &q) {
    //assert(p.is_valid() && p->get_gc_descriptor().is_valid());
    //assert(p.is_valid());
    if (!p.is_valid() || cb.bitmap.is_marked(p) || !p->get_gc_descriptor().is_valid()) {
      return;
    }
    p->get_gc_descriptor().for_each_ref([&q, &cb](const base_offset_ptr base_ptr) {
      if (base_ptr.is_null()) {
        return;
      }
      //nullptr are also invalid!
      //assert(base_ptr.is_valid());
      if (base_ptr.is_valid()) {
        const offset_ptr<const gc_allocated> &ptr = static_cast<const offset_ptr<const gc_allocated>&>(base_ptr);
	assert(ptr->get_gc_descriptor().is_valid());
	if (ptr->get_gc_descriptor().is_valid() && !cb.bitmap.is_marked(ptr)) {
	  //q.push_front(ptr);
          q.push(ptr);
        }
      }
    });
    /*
     * We mark end-bitmap first so that in case we crash before marking begin-bitmap,
     * the object is not considered marked, and whichever process takes over the
     * unfinished work of this process, will mark it.
     */
    cb.bitmap.mark_end_first(p);
    // We should probably do this in the per-process struct and sweep
    // the total into the global when we try to increment the cycle
    // number, but that's more bookkeeping and it might be more work
    // to get the reference to the per-process struct.
    cb.mem_stats.marked(p);
  }

  /*
   * The function to increment the given barrier. For fault-tolerance,
   * the value before incrementing is stored in persistent space as an
   * ID. We cannot use the regular atomic increment, because there is
   * a possibility that increment takes place but ID is not stored in
   * the persistent space. So we use CAS.
   */
  static void inc_barrier(per_process_struct &p, const Barrier_indices n) {
    gc_control_block &cb = control_block();
    pcount_t &i = p.barrier_id_ref();

    p.set_barrier_incrementing();
    assert(i == 0);
    i = cb.barrier_sync[n];
    while (!cb.barrier_sync[n].compare_exchange_weak(i, i + 1));
    //assert(i <= cb.total_process_count.load().count);
    p.set_barrier_incremented();
  }

  /*
   * Function called by GC thread in the marking phase to empty the work queue.
   * We do *push* based work-load balancing, wherein, overloaded thread *offers*
   * work to idle ones.
   */
  static bool empty_collector_stack(gc_control_block &cb, per_process_struct &p, Traversal_queue &q) {
    bool worked = false;
    while (!q.empty()) {
      worked = true;

      p.set_marking_ref(q.front());
      q.pop();
      mark_black(p.marking_ref(), cb, q);
      p.clear_marking_ref();
      if (request_gc_termination) {
        break;
      }
    }
    return worked;
  }

  static void consume_dead_process_refs(per_process_struct &process_struct, Traversal_queue &my_q) {
    Traversal_queue &q = process_struct.traversal_queue();
    Mark_buffer_list &mb_list = process_struct.mark_buffer_list();
    Mbuf *m = mb_list.head();
    while (m) {
      while (!m->is_empty()) {
        m->process_element([&my_q] (const offset_ptr<const gc_allocated> &p) {
          my_q.push(p);
        });
      }
      m = mb_list.next(m);
    }
    my_q.push(process_struct.marking_ref());
    my_q.takeover_locals(q);
  }

  template <typename Func, typename ...Args>
  static pcount_t cleanup_failures(void (*dead_action)(per_process_struct*), Func &&cleanup_func, Args&& ...args) {
    gc_control_block &cb = control_block();
    pcount_t nr_dead_process = 0;
    pcount_t nr_total_process = 0;
    per_process_struct *p = gc_handshake::process_struct;
    barrier_id_dead_processes_map<per_process_struct*> map;

    assert(!map[p->get_barrier_id()].found);
    map[p->get_barrier_id()].found = true;
    do {
      nr_total_process++;
      p = cb.process_struct_list.next(p);
      if (p == nullptr) {
        p = cb.process_struct_list.head();
      }
      if (p == gc_handshake::process_struct) {
        break;
      } else if (request_gc_termination) {
        return 0;
      }

      per_process_struct::liveness old_liveness = p->get_liveness();
      per_process_struct::Barrier_info binfo = p->get_barrier_info();

      if (old_liveness.is_live == per_process_struct::Alive::Dead) {
        nr_dead_process++;
        continue;
      } else if (binfo._info._barrier_idx == next_barrier_index_mapping[gc_handshake::process_struct->get_barrier_index()]) {
        return 0;
      } else if (per_process_struct::get_creation_time(old_liveness.pid) != old_liveness.creation_time) {
        if (binfo._info._barrier_idx != gc_handshake::process_struct->get_barrier_index()) {
          //The following commented code is required only if there is a possibility of the same barrier is used back-to-back.
          //proc.reset_barrier_info(proc.get_barrier_index());
          dead_action(p);
          nr_dead_process++;
          continue;
        } else if (binfo._info._bstage == per_process_struct::Barrier_stage::unincremented) {
          if (std::forward<Func>(cleanup_func)(p, old_liveness, std::forward<Args>(args)...)) {
            //The function is suppose to return true if dead process count must be incremented.
            nr_dead_process++;
          }
          continue;
        } else if (binfo._info._bstage == per_process_struct::Barrier_stage::incrementing) {
          barrier_id_dead_processes_t<per_process_struct*> &barrier_id_value = map[binfo._info._barrier.barrier];
          if (barrier_id_value.found) {
            nr_dead_process++;
            dead_action(p);
          } else {
            barrier_id_value.deque.push_front(p);
          }
          continue;
        }
      }
      //if it has incremented already, it should be accounted for.
      if (binfo._info._bstage == per_process_struct::Barrier_stage::incremented &&
          binfo._info._barrier_idx == gc_handshake::process_struct->get_barrier_index()) {
        barrier_id_dead_processes_t<per_process_struct*> &barrier_id_value = map[binfo._info._barrier.barrier];
        assert(!barrier_id_value.found);
        barrier_id_value.found = true;
        nr_dead_process += barrier_id_value.deque.size();
        while(!barrier_id_value.deque.empty()) {
          per_process_struct *proc = barrier_id_value.deque.front();
          dead_action(proc);
          barrier_id_value.deque.pop_front();
        }
      }
    } while (true);

    pcount_t curr_barrier_count = cb.barrier_sync[gc_handshake::process_struct->get_barrier_index()];
    while (!map.empty()) {
      barrier_id_dead_processes_t<per_process_struct*> &barrier_id_value = map.begin()->second;
      if (!barrier_id_value.found) {
        assert(barrier_id_value.deque.size() > 0);
        nr_dead_process += barrier_id_value.deque.size() - 1;
        if (map.begin()->first == curr_barrier_count) {
          nr_dead_process++;
          while(!barrier_id_value.deque.empty()) {
            per_process_struct *proc = barrier_id_value.deque.front();
            dead_action(proc);
            barrier_id_value.deque.pop_front();
          }
        }
      }
      map.erase(map.begin());
    }
    return nr_total_process - nr_dead_process;
  }

  static pcount_t cleanup_marking_failures(gc_control_block &cb, Traversal_queue &my_q) {
    pcount_t nr_dead_process = 0;
    pcount_t nr_total_process = 0;
    per_process_struct *p = gc_handshake::process_struct;
    using map_pair = std::pair<per_process_struct*, per_process_struct::liveness>;
    barrier_id_dead_processes_map<map_pair> map;
    assert(!map[p->get_barrier_id()].found);
    map[p->get_barrier_id()].found = true;
    do {
      nr_total_process++;
      p = cb.process_struct_list.next(p);
      if (p == nullptr) {
        p = cb.process_struct_list.head();
      }
      if (p == gc_handshake::process_struct) {
        break;
      } else if (request_gc_termination) {
        return 0;
      }

      per_process_struct::liveness old_liveness = p->get_liveness();
      per_process_struct::Barrier_info binfo = p->get_barrier_info();
      if (old_liveness.is_live == per_process_struct::Alive::Dead) {
        nr_dead_process++;
        continue;
      } else if (binfo._info._barrier.version > gc_handshake::process_struct->get_barrier_version() ||
                 (binfo._info._barrier.version == gc_handshake::process_struct->get_barrier_version() &&
                  binfo._info._barrier_idx == Barrier_indices::marking2)) {
        return 0;
      } else if (per_process_struct::get_creation_time(old_liveness.pid) != old_liveness.creation_time) {
        per_process_struct &proc = *p;
        per_process_struct::liveness desired = gc_handshake::process_struct->get_liveness();
        if (proc.set_liveness(old_liveness, desired)) {
          consume_dead_process_refs(proc, my_q);
          bool ret = proc.set_liveness(desired, old_liveness);
          assert(ret);
        }
        if (binfo._info._barrier.version < gc_handshake::process_struct->get_barrier_version() ||
            (binfo._info._barrier_idx != Barrier_indices::marking1 && binfo._info._barrier_idx != Barrier_indices::marking2)) {
          //The following commented code is required only if there is
          //a possibility of the same barrier is used back-to-back.

          //proc.reset_barrier_info(proc.get_barrier_index());

          //In this function, if we fail to mark_dead means somebody
          //either is still working on cleaning up a dead process, or
          //has died while working on it. Therefore, at anytime this
          //function returns false, we must return out and try fresh.
          if (!proc.mark_dead(old_liveness)) {
            return 0;
          }
          nr_dead_process++;
          continue;
        } else if (binfo._info._barrier_idx == Barrier_indices::marking2) {
          assert(false);
        } else {
          assert(binfo._info._barrier_idx == Barrier_indices::marking1);
          assert(binfo._info._barrier.version == gc_handshake::process_struct->get_barrier_version());
          if (binfo._info._bstage == per_process_struct::Barrier_stage::unincremented) {
            if (!proc.mark_dead(old_liveness)) {
              return 0;
            }
            nr_dead_process++;
            continue;
          } else if (binfo._info._bstage == per_process_struct::Barrier_stage::incrementing) {
            barrier_id_dead_processes_t<map_pair> &barrier_id_value = map[binfo._info._barrier.barrier];
            if (barrier_id_value.found) {
              if (proc.mark_dead(old_liveness)) {
                return 0;
              }
              nr_dead_process++;
            } else {
              barrier_id_value.deque.push_front(map_pair(p, old_liveness));
            }
            continue;
          }
        }
      }

      //if it has incremented already, it should be accounted for.
      if (binfo._info._bstage == per_process_struct::Barrier_stage::incremented &&
          binfo._info._barrier.version == gc_handshake::process_struct->get_barrier_version() &&
          binfo._info._barrier_idx == Barrier_indices::marking1) {
        //For processes with version < our version, consider it as unincremented and hence don't do
        //anything, just continue to next process.
        barrier_id_dead_processes_t<map_pair> &barrier_id_value = map[binfo._info._barrier.barrier];
        assert(!barrier_id_value.found);
        barrier_id_value.found = true;
        nr_dead_process += barrier_id_value.deque.size();
        while(!barrier_id_value.deque.empty()) {
          map_pair &pair = barrier_id_value.deque.front();
          if (!pair.first->mark_dead(pair.second)) {
            return 0;
          }
          barrier_id_value.deque.pop_front();
        }
      }
    } while (true);

    marking1_barrier_t curr_barrier_count = cb.marking1_barrier;
    while (!map.empty()) {
      barrier_id_dead_processes_t<map_pair> &barrier_id_value = map.begin()->second;
      if (!barrier_id_value.found) {
        assert(barrier_id_value.deque.size() > 0);
        nr_dead_process += barrier_id_value.deque.size() - 1;
        if (map.begin()->first == curr_barrier_count.barrier) {
          nr_dead_process++;
          while(!barrier_id_value.deque.empty()) {
            map_pair &pair = barrier_id_value.deque.front();
            if (!pair.first->mark_dead(pair.second)) {
              return 0;
            }
            barrier_id_value.deque.pop_front();
          }
        }
      }
      map.erase(map.begin());
    }
    return nr_total_process - nr_dead_process;
  }

  static bool help_other_processes(gc_control_block &cb, per_process_struct &process_struct, Traversal_queue &q) {
    per_process_struct *p = &process_struct;
    bool helped = false;
    do {
      p = cb.process_struct_list.next(p);
      if (p == nullptr) {
        p = cb.process_struct_list.head();
      }
      if (p == gc_handshake::process_struct) {
        break;
      } else if (request_gc_termination) {
        //Return true to terminate at the earliest possible.
        return helped;
      }

      while (p->steal(q)) {
        helped = true;
        empty_collector_stack(cb, process_struct, q);
        if (request_gc_termination) {
          return helped;
        }
      }
    } while (true);
    return helped;
  }

  static void marking_phase(per_process_struct &process_struct) {
    gc_control_block &cb = control_block();
    Traversal_queue &q = process_struct.traversal_queue();
    Mark_buffer_list &mb_list = process_struct.mark_buffer_list();
    marking1_barrier_t local_marking1_barrier;
    versioned_pcount_t nr_live_process;
    pcount_t curr_version;
    bool clean = false;
    uint8_t spin_count;

    do {
      process_struct.reset_barrier_info(Barrier_indices::marking1);
      do {
        if (request_gc_termination) {
          return;
        }
        while (!clean) {
          clean = true;
          Mbuf *m = mb_list.head();
          while (m) {
            while (!m->is_empty()) {
              clean = false;
              m->process_element(mark_black, cb, q);
              if (request_gc_termination) {
                return;
              }
            }
            m = mb_list.next(m);
          }
          empty_collector_stack(cb, process_struct, q);
        }
        // Let's help others.
        if (!help_other_processes(cb, process_struct, q)) {
          // Nobody needed help.
          break;
        }
        /* Spent some time helping others. Let's check again if
         * some mutator(s) have unfinished business.
         */
      } while(true);

      spin_count = 0;
      assert(clean);
      if (request_gc_termination) {
        return;
      }

      {
        marking1_barrier_t &desired = local_marking1_barrier;
        marking1_barrier_t &expected = process_struct.marking1_barrier_ref();
        process_struct.set_barrier_incrementing();
        expected = cb.marking1_barrier;
        curr_version = expected.version;
        do {
          desired = expected;
          desired.barrier++;
          //assert(desired.barrier <= cb.total_process_count.load().count);
          assert(curr_version == expected.version);
        } while (!cb.marking1_barrier.compare_exchange_weak(expected, desired));
        process_struct.set_barrier_incremented();
      }

      /* We don't need to check for stage == Stage::Tracing here
       * because there is no way that any GC thread goes past Marking2
       * barrier.
       */
      nr_live_process = cb.total_process_count;
      while (local_marking1_barrier.barrier < nr_live_process.count && local_marking1_barrier.version == curr_version) {
        assert(cb.stage == Stage::Tracing);
        if (++spin_count == 0) {
          // First help others.
          if (!help_other_processes(cb, process_struct, q)) {
            // If nobody needed help, then check for failures.
            versioned_pcount_t temp_live_process(cleanup_marking_failures(cb, q), nr_live_process.version + 1);
            //temp_live_process = 0 indicates that termination has been requested.
            if (temp_live_process.count > 0 && cb.total_process_count.compare_exchange_strong(nr_live_process, temp_live_process)) {
              nr_live_process = temp_live_process;
            }
            /* Set clean to false if work done. Otherwise, leave it as is.
             * It's important to have empty_collector_stack as the first
             * operand to && so that the function is called no matter what.
             */
            clean = !empty_collector_stack(cb, process_struct, q) && clean;
          } else {
            /* This needs some explanation! Consider a scenario where GC thread 1
             * after incrementing marking barrier 1 steals work from GC thread 2.
             * This may cause GC thread 2 to think that its work is done and hence
             * will increment marking barrier 1, which may meet the marking barrier 1's
             * criteria. Thereafter, GC thread 2 will check its mark-buffers, find them
             * empty and then increment marking barrier 2. At this time, GC thread 1
             * may be working on the stolen work. But during the same time, if some
             * mutator(s) in the GC thread 2's process update(s) a reference among
             * white references, it will cause references to be added to the mark-
             * buffer(s), which may go unnoticed because its GC thread is already
             * at marking barrier 2 and GC thread 1 will eventually finish the stolen
             * work, may find its mark-buffers empty, and hence increment the marking
             * barrier 2.
             *
             * To solve this, we make GC thread 1 pretend as if it found in its mark
             * buffers after exiting marking barrier 1. This will make all GC threads
             * to go back for another round of marking, when GC thread 2 will find
             * unprocessed references in marking buffers, if any.
             */
            clean = false;
          }
        }

        if (request_gc_termination) {
          return;
        }
        std::cpu_relax();
        nr_live_process = cb.total_process_count;
        local_marking1_barrier = cb.marking1_barrier;
      }

      assert(q.empty());
      process_struct.reset_barrier_info(Barrier_indices::marking2);
      if (local_marking1_barrier.version != curr_version) {
        /* This indicates that somebody has already incremented the version.
         * We don't need to do anything now. Just continue.
         */
        clean = false;
        continue;
      } else if (clean) {
        for (Mbuf *m = mb_list.head(); m; m = mb_list.next(m)) {
          if (!m->is_empty()) {
            clean = false;
            break;
          }
        }
      }

      if (request_gc_termination) {
        return;
      }

      if (!clean) {
        marking1_barrier_t desired;
        desired.version = local_marking1_barrier.version + 1;
        while (local_marking1_barrier.version != desired.version &&
               !cb.marking1_barrier.compare_exchange_strong(local_marking1_barrier, desired));
      } else {
        spin_count = 0;
        inc_barrier(process_struct, Barrier_indices::marking2);
        while (cb.barrier_sync[Barrier_indices::marking2] < cb.total_process_count.load().count && cb.stage == Stage::Tracing) {
          std::cpu_relax();
          local_marking1_barrier = cb.marking1_barrier;
          if (local_marking1_barrier.version != curr_version) {
            assert(local_marking1_barrier.barrier < cb.total_process_count.load().count);
            clean = false;
            cb.barrier_sync[Barrier_indices::marking2] = 0;
            break;
          } else if (++spin_count == 0) {
            pcount_t temp_live_process = cleanup_failures([](per_process_struct *p) {return;},
                                                          [](per_process_struct *p, 
                                                             per_process_struct::liveness &expected) {return true;});
            if (temp_live_process > 0 && cb.total_process_count.load().count != temp_live_process) {
              //We are here because curr_version == local_marking1_barrier.version.
              marking1_barrier_t desired;
              desired.version = local_marking1_barrier.version + 1;
              while (local_marking1_barrier.version != desired.version &&
                     !cb.marking1_barrier.compare_exchange_strong(local_marking1_barrier, desired));
            }
          }
          if (request_gc_termination) {
            return;
          }
        }
      }
    } while(!clean);
    process_struct.reset_barrier_info(Barrier_indices::preSweep);

    assert(q.empty());
    for (Mbuf *m = mb_list.head(); m; m = mb_list.next(m)) {
      while (!m->is_empty()) {
        m->process_element([&cb] (const offset_ptr<const gc_allocated> &p) {
          assert(cb.bitmap.is_marked(p));
        });
      }
    }
  }

  static void erase_gc_descriptors_from_free_chunk(offset_ptr<gc_allocated> begin,
                                                   const offset_ptr<gc_allocated>::difference_type size_in_words) {
    assert(sizeof(std::size_t) == sizeof(gc_allocated));
    const offset_ptr<const gc_allocated> end = begin + size_in_words;
    const std::size_t heap_size = base_offset_ptr::heap_size();

    for (std::size_t size = 0; begin < end; begin += size) {
      std::size_t *b = reinterpret_cast<std::size_t*>(begin.as_bare_pointer());
      /*
       * The following logic works because object_descriptor is structured such
       * that it will be always greater than any possible object size.
       */
      if (*b > heap_size) {
        size = begin->get_gc_descriptor().object_size();
        *b = size << 3;
      } else {
        size = *b >> 3;
      }
      assert(size != 0);
    }
    assert(begin == end);
  }

  static void put_to_global(gc_allocator::globalListType& list, const std::size_t beg_word, const std::size_t size_in_bytes) {
    if (size_in_bytes == 0) {
      return;
    }
    std::size_t *begin = reinterpret_cast<std::size_t*>(base_offset_ptr::base()) + beg_word;
    if (size_in_bytes < sizeof(gc_allocator::global_chunk)) {
      *begin = size_in_bytes;
      return;
    }
    erase_gc_descriptors_from_free_chunk(reinterpret_cast<gc_allocated*>(begin), size_in_bytes >> 3);

    gc_allocator::put_to_global(list, size_in_bytes,
                                new (begin) gc_allocator::global_chunk(size_in_bytes));
  }

  void sweep1_phase() {
    gc_control_block &cb = control_block();
    gc_allocator::globalListType &other_list = cb.global_free_list[1 - gc_handshake::process_struct->global_list_index()];
    Pre_sweep_list &pre_sweep_list = gc_handshake::process_struct->pre_sweep_list();
    bool work_done = false;
    assert(pre_sweep_list.empty());
    for (uint8_t i = gc_allocator::global_list_size(); !work_done && i > 5; i--) {
      do {
        if (request_gc_termination) {
          return;
        }
        offset_ptr<gc_allocator::global_chunk> c = gc_allocator::get_chunk_from_global(other_list, i - 1);
        if (c == nullptr) {
          break;
        } else {
          work_done = true;
        }
        std::size_t beg_word, end_word;
        if (cb.bitmap.expand_free_chunk(c, c->size(), beg_word, end_word)) {
          /* We have to be carefull here. We are pushing beg_word and end_word as
           * two different iteruts in the deque. But they are really meant for single
           * operation. Therefore, while taking them out, firstly, the size of deque
           * should be in multiples of two. And, secondly, two pop operations should
           * be done from the other front to retrieve the beg_word and end_word in
           * right order.
           */
          pre_sweep_list.push_back(beg_word);
          pre_sweep_list.push_back(end_word);
        }
      } while(true);
    }
  }

  void mark_bitmap::process_logical_chunk(gc_allocator::globalListType &list, const std::size_t nr_chunk, const bool set_bit) {
    std::size_t first = 0;
    const std::size_t end = (nr_chunk + 1) << (chunk_size_log_bits + value_log_bits);
    std::size_t second;

    if (nr_chunk == 0) {
      second = find_next_used_word(first, end);
      if (second == end) {
        set_sweep_bitmap_both(0, set_bit);
        second = process_next_chunk_begin(1, set_bit);
        put_to_global(list, first, (second - first) << 3);
        return;
      } else {
        put_to_global(list, first, (second - first) << 3);
      }
    } else {
      second = nr_chunk << (chunk_size_log_bits + value_log_bits);
    }

    bool dirty_end_bitmap = false;
    while (second < end) {
      first = find_next_free_word(second, end, dirty_end_bitmap);
      if (first == end) {
        rep_t B = _end[((nr_chunk + 1) << chunk_size_log_bits) - 1];
        if (B & 0x1) {
          //If the last word of this chunk was end of an object, then we have to process the next chunk's _begin
          second = process_next_chunk_begin(nr_chunk + 1, set_bit);
        } else {
          break;//There is nothing to add in the free list.
        }
      } else {
        second = find_next_used_word(first, end);
        if (second == end) {
          second = process_next_chunk_begin(nr_chunk + 1, set_bit);
        }
      }
      put_to_global(list, first, (second - first) << 3);
    }

    if (!dirty_end_bitmap) {
      set_sweep_bitmap_end(nr_chunk, set_bit);
    }
  }

  void mark_bitmap::_set_sweep_bitmap_range(const std::size_t start_chunk, const std::size_t finish_chunk, const bool set) {
    std::size_t start_word_idx = start_chunk >> value_log_bits;
    const std::size_t finish_word_idx = finish_chunk >> value_log_bits;

    const rep_t first_chunk_desired = (rep_t(1) << (bits_per_value - (start_chunk & (bits_per_value - 1)))) - 1;
    const rep_t last_chunk_desired = ~(construct_bitmap_word(finish_chunk & (bits_per_value - 1)) - 1);

    if (start_word_idx == finish_word_idx) {
      set_sweep_bitmap(_sweep_bitmap_end[start_word_idx], first_chunk_desired & last_chunk_desired, set);
      set_sweep_bitmap(_sweep_bitmap_begin[start_word_idx], first_chunk_desired & last_chunk_desired, set);
      return;
    }

    set_sweep_bitmap(_sweep_bitmap_end[start_word_idx], first_chunk_desired, set);
    set_sweep_bitmap(_sweep_bitmap_begin[start_word_idx], first_chunk_desired, set);

    const rep_t word_to_write = set ? rep_t(-1) : 0;
    for (start_word_idx++; start_word_idx < finish_word_idx; start_word_idx++) {
      _sweep_bitmap_end[start_word_idx] = word_to_write;
      _sweep_bitmap_begin[start_word_idx] = word_to_write;
    }

    set_sweep_bitmap(_sweep_bitmap_end[finish_word_idx], last_chunk_desired, set);
    set_sweep_bitmap(_sweep_bitmap_begin[finish_word_idx], last_chunk_desired, set);
  }

  void mark_bitmap::set_sweep_bitmap_range(const std::size_t beg_word, const std::size_t end_word, const bool set_bit) {
    const std::size_t begin_chunk = beg_word >> (chunk_size_log_bits + value_log_bits);
    const std::size_t end_chunk = end_word >> (chunk_size_log_bits + value_log_bits);
    if (end_chunk - begin_chunk < 2) {
      //Nothing to do;
      return;
    }
    _set_sweep_bitmap_range(begin_chunk + 1, end_chunk - 1, set_bit);
  }

  void mark_bitmap::sweep2_phase(const bool set_bitmap) {
    assert(gc_handshake::process_struct->get_tolerate_sweep_chunk() == 0);
    std::size_t &i = gc_handshake::process_struct->get_tolerate_sweep_chunk();
    gc_control_block &cb = control_block();
    gc_allocator::globalListType &list = cb.global_free_list[gc_handshake::process_struct->global_list_index()];

    {
      Pre_sweep_list &pre_sweep_list = gc_handshake::process_struct->pre_sweep_list();
      assert(pre_sweep_list.size() % 2 == 0);
      while (!pre_sweep_list.empty()) {
        std::size_t beg_word = pre_sweep_list.front();
        pre_sweep_list.pop_front();
      
        std::size_t end_word = pre_sweep_list.front();
        pre_sweep_list.pop_front();

        put_to_global(list, beg_word, (end_word - beg_word) << 3);

        if (request_gc_termination) {
          return;
        }
        /* We can set all the bits within the chunk to be set as they will not
         * have any dirty chunk.
         */
        cb.bitmap.set_sweep_bitmap_range(beg_word, end_word - 1, set_bitmap);
      }
    }

    do {
      if (request_gc_termination) {
        break;
      }
      fetch_logical_chunk_to_process(i);
      if (i >= _total_logical_chunks) {
        break;
      }
      if (!is_end_sweep_bitmap_set(i, set_bitmap)) {
        process_logical_chunk(list, i, set_bitmap);
      }
    } while (true);
    //The following clearing of the other global allocator will not be required once we have the optimized sweep code.
    gc_allocator::globalListType &other_list = cb.global_free_list[1 - gc_handshake::process_struct->global_list_index()];
    for (uint8_t i = 0; i < gc_allocator::global_list_size(); i++) {
      other_list[i].store(gc_allocator::list_head());
      assert(other_list[i].load().empty());
    }
  }

  void mark_bitmap::_post_sweep_clear(atomic_rep_t &word, atomic_rep_t * bitmap_chunk, const bool set_bit) {
    rep_t val = word;
    rep_t iter = construct_bitmap_word(0);
    const std::size_t size = sizeof(atomic_rep_t) << chunk_size_log_bits;
    if (!set_bit) {
      val = ~val;
    }
    while (iter) {
      if (!(val & iter)) {
        std::memset(bitmap_chunk, 0x0, size);
        set_sweep_bitmap(word, iter, set_bit);
      }
      iter >>= 1;
      bitmap_chunk += 1 << chunk_size_log_bits;
    }
  }

  void mark_bitmap::post_sweep_clear(const std::size_t nr_sweep_bitmap_word, const bool set_bit) {
    if (nr_sweep_bitmap_word >= _total_logical_chunks) {
      /* This is possible if a process terminates after finishing
       * sweep_phase2 but before fetching first bitmap word.
       */
      return;
    }
    assert(nr_sweep_bitmap_word < _sweep_bitmap_size);
    const std::size_t word_begin = nr_sweep_bitmap_word << (value_log_bits + chunk_size_log_bits);
    _post_sweep_clear(_sweep_bitmap_begin[nr_sweep_bitmap_word], _begin + word_begin, set_bit);
    _post_sweep_clear(_sweep_bitmap_end[nr_sweep_bitmap_word], _end + word_begin, set_bit);
  }

  void mark_bitmap::post_sweep_phase(per_process_struct *process_struct, const bool set_bit) {
    assert(gc_handshake::process_struct->get_tolerate_sweep_chunk() >= _total_logical_chunks);

    std::size_t &i = gc_handshake::process_struct->get_tolerate_sweep_chunk();
    do {
      if (request_gc_termination) {
        break;
      }
      fetch_sweep_bitmap_word_to_process(i);
      if (i >= _sweep_bitmap_size) {
        break;
      }
      post_sweep_clear(i, set_bit);
    } while (true);
  }

  static bool cleanup_sweep1_phase(per_process_struct *p, per_process_struct::liveness &expected) {
    per_process_struct::liveness desired = gc_handshake::process_struct->get_liveness();
    if (p->set_liveness(expected, desired)) {
      Pre_sweep_list &dead_list = p->pre_sweep_list();
      Pre_sweep_list &my_list = gc_handshake::process_struct->pre_sweep_list();

      /* If the size of dead_list is odd, that means the dead process couldn't
       * push both begin and end. In that case, simply get rid of the begin that
       * was pushed at the back.
       *
       * There are two scenarios in which the dead_list's size can be odd:
       * 1) If the process to which dead_list belong, died after adding the
       * beginning of the free range, but before adding the end of the range.
       * In this case, we must throw away the back.
       *
       * 2) If some process was cleaning up the dead_list, but died after popping
       * the beginning of a free range, but before popping the end. In this case,
       * The front of the deque will be -1. In this case, throw away the front.
       */
      if (dead_list.size() % 2 != 0) {
        if (dead_list.front() == std::size_t(-1)) {
          dead_list.pop_front();
        } else {
          dead_list.pop_back();
        }
      }

      /* In the loop below, there is a very short window wherein we might have
       * element next to the front one set to -1. At this point, if a process
       * dies, we need a way to recover out of it. So, we throw away the first
       * two elements.
       */
      if (dead_list.size() && dead_list[1] == std::size_t(-1)) {
        dead_list.pop_front();
        dead_list.pop_front();
      }

      while (!dead_list.empty()) {
        std::size_t first = dead_list[0];
        std::size_t second = dead_list[1];

        dead_list[1] = std::size_t(-1);
        //The popping of elements should happen only after -1 assignment above.
        std::atomic_signal_fence(std::memory_order_release);
        dead_list.pop_front();
        dead_list.pop_front();
        //The pushing of the two elements must happen only after popping above.
        std::atomic_signal_fence(std::memory_order_release);
        my_list.push_back(first);
        my_list.push_back(second);
      }

      expected.is_live = per_process_struct::Alive::Dead;
      bool ret = p->set_liveness(desired, expected);
      assert(ret);
      return true;
    }
    return false;
  }

  /*
   * Cleanup function called for a crashed process for recovery. After cleanup,
   * it restores the ownserhip of the dead process' structure that is taken
   * before the cleanup.
   */
  static bool cleanup_post_sweep_phase(per_process_struct *p, per_process_struct::liveness &expected, const bool set_bit) {
    per_process_struct::liveness desired = gc_handshake::process_struct->get_liveness();
    if (p->set_liveness(expected, desired)) {
      control_block().bitmap.post_sweep_clear(p->get_tolerate_sweep_chunk(), set_bit);
      p->reset_tolerate_sweep_chunk();
      expected.is_live = per_process_struct::Alive::Dead;
      bool assert_test = p->set_liveness(desired, expected);
      assert(assert_test);
      return true;
    }
    return false;
  }

  static void synchronize_gc_threads(const Barrier_indices n, Stage stage, const bool set_bit = 0) {
    gc_control_block &cb = control_block();
    uint16_t spin_count = 0;//We should may be try not initializing it, to incorporate some randomness.
    auto action_on_dead_process = [](per_process_struct *p) { p->mark_dead(); };

    inc_barrier(*gc_handshake::process_struct, n);

    versioned_pcount_t nr_live_process = cb.total_process_count;
    while(cb.barrier_sync[n] < nr_live_process.count && cb.stage == stage) {
      std::cpu_relax();
      if (++spin_count > 0) {
        nr_live_process = cb.total_process_count;
        continue;
      }
      pcount_t temp_live_process;
      switch(n) {
      case Barrier_indices::sync:
      case Barrier_indices::preMarking:
      case Barrier_indices::preSweep:
      case Barrier_indices::sweep2:
        temp_live_process = cleanup_failures(action_on_dead_process,
                                             [](per_process_struct *p, per_process_struct::liveness &expected) -> bool {
            p->mark_dead();
            return true;
          });
        break;
      case Barrier_indices::sweep1:
        temp_live_process = cleanup_failures(action_on_dead_process, cleanup_sweep1_phase);
        break;
      case Barrier_indices::postSweep:
        temp_live_process = cleanup_failures(action_on_dead_process, cleanup_post_sweep_phase, set_bit);
        break;
      default:
        /* marking1 and marking2 will not come here as they are related to marking and
         * dealt with separately in marking_phase() function.
         */
        assert(false);
      }
      if (request_gc_termination) {
        return;
      }
      if (temp_live_process > 0 &&
          cb.total_process_count.compare_exchange_strong(nr_live_process,
                                                         versioned_pcount_t(temp_live_process,
                                                                            nr_live_process.version + 1))) {
        nr_live_process.count = temp_live_process;
        nr_live_process.version++;
      }
    }

    gc_handshake::process_struct->reset_barrier_info(next_barrier_index_mapping[n]);
  }

  /*
   * Asserts during sweep signal that the new allocator list is completely empty.
   */
  void assert_current_alloc_list_empty() {
    gc_allocator::globalListType &list =
          control_block().global_free_list[1 - gc_handshake::thread_struct_handles.handle->status_idx.load().index()];
    for (uint8_t i = 0; i < gc_allocator::global_list_size(); i++) {
      assert(list[i].load().empty());
    }
  }


  void trace_gc_cycle(int n, const gc_control_block &cb) {
    static const bool tracep = ruts::env_flag("GC_TRACE_CYCLES");
    if (tracep) {
      std::string suffix
        = (n%10==1 && n%100!=11) ? "st"
        : (n%10==2 && n%100!=12) ? "nd"
        : (n%10==3 && n%100!=13) ? "rd"
        : "th";
      const gc_mem_stats &ms = cb.mem_stats;
      std::cout << "Processing " << n << suffix << " iteration"
                << " [" << ms.cycle_number() << ": "
                << ms.bytes_in_use() << " bytes marked of "
                << ms.bytes_in_heap()
                << ", " << ms.n_processes() << " process"
                << (ms.n_processes()==1 ? "" : "es")
                << "]" << std::endl;
    }
  }
  
  void start_gc(Stage local_stage) {
    gc_control_block &cb = control_block();
    int count = 0;
    std::size_t gc_cycle_num = cb.mem_stats.cycle_number();
    gc_status local_status = gc_handshake::process_struct->get_gc_status();

    //Following switch-case is to fix the barrier info to contain right barrier index.
    switch (local_stage) {
    case Stage::Sweeped:
    case Stage::preTracing:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::sync);
      break;
    case Stage::Tracing:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::preMarking);
      break;
    case Stage::Traced:
    case Stage::preSweeping:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::preSweep);
      break;
    case Stage::Sweeping:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::sweep1);
    }

    while (true) {
      trace_gc_cycle(count, cb);
      switch (local_stage) {
      case Stage::Sweeped: {
        local_status.status_idx.status = gc_handshake::Signum::sigSweep;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigSync1,
                                              local_status.status_idx.idx))) {
          local_status.status_idx.status = gc_handshake::Signum::sigSync1;
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        gc_handshake::handshake(gc_handshake::Signum::sigSync1);
        if (request_gc_termination) {
          break;
        }
        //barriers used for marking can be cleaned at this point
        cb.marking1_barrier = marking1_barrier_t();
        cb.barrier_sync[Barrier_indices::marking2] = 0;

        cb.stage.compare_exchange_strong(local_stage, Stage::preTracing);
        local_stage = Stage::preTracing;
      }
      case Stage::preTracing: {
        //All GC threads must synchronize at this point.
        synchronize_gc_threads(Barrier_indices::sync, local_stage);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::preSweep] = 0;
        cb.barrier_sync[Barrier_indices::sweep1] = 0;

        local_status.status_idx.status = gc_handshake::Signum::sigSync1;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigSync2,
                                              local_status.status_idx.idx))) {
          local_status.status_idx.status = gc_handshake::Signum::sigSync2;
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        gc_handshake::handshake(gc_handshake::Signum::sigSync2);
        if (request_gc_termination) {
          break;
        }

        cb.stage.compare_exchange_strong(local_stage, Stage::Tracing);
        local_stage = Stage::Tracing;
      }
      case Stage::Tracing: {
        //All GC threads must synchronize at this point.
        synchronize_gc_threads(Barrier_indices::preMarking, local_stage);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::sweep2] = 0;
        cb.barrier_sync[Barrier_indices::postSweep] = 0;
        cb.bitmap.reset_logical_chunk_count();

        local_status.status_idx.status = gc_handshake::Signum::sigSync2;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigAsync,
                                                        local_status.status_idx.idx))) {
          local_status.status_idx.status = gc_handshake::Signum::sigAsync;
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        gc_handshake::post_handshake(gc_handshake::Signum::sigAsync);
        capture_global_roots(gc_handshake::process_struct->traversal_queue());
        gc_handshake::wait_handshake(gc_handshake::Signum::sigAsync);
        if (request_gc_termination) {
          break;
        }

        marking_phase(*gc_handshake::process_struct);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::sync] = 0;
        //No synchronization required at this point as already done in marking_phase(process_struct)

        cb.stage.compare_exchange_strong(local_stage, Stage::Traced);
        local_stage = Stage::Traced;
      }
      case Stage::Traced: {
        local_status.status_idx.status = gc_handshake::Signum::sigAsync;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigSweep,
                                                        1 - local_status.status_idx.idx))) {
          local_status = gc_status(gc_handshake::Signum::sigSweep, 1 - local_status.status_idx.idx);
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        gc_handshake::handshake(gc_handshake::Signum::sigSweep);
        if (request_gc_termination) {
          break;
        }

        cb.stage.compare_exchange_strong(local_stage, Stage::preSweeping);
        local_stage = Stage::preSweeping;
      }
      case Stage::preSweeping: {
        //All GC threads must synchronize at this point.
        synchronize_gc_threads(Barrier_indices::preSweep, local_stage);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::preMarking] = 0;

        sweep1_phase();

        cb.stage.compare_exchange_strong(local_stage, Stage::Sweeping);
        local_stage = Stage::Sweeping;
      }
      case Stage::Sweeping: {
        synchronize_gc_threads(Barrier_indices::sweep1, local_stage);
        if (request_gc_termination) {
          break;
        }

        cb.bitmap.sweep2_phase(local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }

        synchronize_gc_threads(Barrier_indices::sweep2, local_stage, local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }

        cb.bitmap.post_sweep_phase(gc_handshake::process_struct, local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }

        synchronize_gc_threads(Barrier_indices::postSweep, local_stage, local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }
        gc_handshake::process_struct->reset_tolerate_sweep_chunk();
        //cb.bitmap.test_bitmaps(local_status.status_idx.idx);

        cb.stage.compare_exchange_strong(local_stage, Stage::Sweeped);
        local_stage = Stage::Sweeped;

        gc_handshake::thread_struct_list.deletion(gc_handshake::in_memory_thread_struct::is_marked);
        cb.process_struct_list.deletion(gc_handshake::process_struct, per_process_struct::is_marked);
        gc_handshake::process_struct->clear();
      }
      } //switch-case statement

      if (request_gc_termination) {
        {
          std::lock_guard<std::mutex> lk(gc_termination_mutex);
          request_gc_termination = false;
        }
        gc_terminated.notify_all();
        return;
      }
      count++;
      // This may not be the appropriate place to put this.  We want
      // it at a point where nobody's marking yet and everybody's
      // finished marking.
      gc_cycle_num = cb.mem_stats.inc_cycle_num_to(gc_cycle_num+1);
    } //while(true)
  }

  /*
   * atexit handler. Helps in GC to gracefully terminate when process
   * termination is requested.
   */
  void atexit_gc_handler() {
    request_gc_termination = true;
    std::unique_lock<std::mutex> lk(gc_termination_mutex);
    gc_terminated.wait(lk, []{return !request_gc_termination;});
  }
}
